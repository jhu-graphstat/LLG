\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage{theorem}
\usepackage{amssymb}
\usepackage{bm}

\usepackage[colorlinks=TRUE,linkcolor=olive,citecolor=brown]{hyperref}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}

\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}

\usepackage{algorithm}
\usepackage{algorithmic}

\makeatletter
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z@\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother

\newcommand{\RT}[1]{\marginpar{\footnotesize\color{red}RT: #1}}

\newcommand{\Ex}{\mathbb{E}}

\bibliographystyle{plainnat}

\title{Law of Large Graphs}

\date{\today}

\begin{document}
\maketitle

\section{Introduction}

Estimation of the mean of a population based on a sample is at the core of statistics.
The sample mean, motivated by the law of large numbers and the central limit theorem, has its place as one of the most important statistics for this task.
Nowadays we take averages almost everywhere, from data in Euclidean space to more complex objects, like shapes, documents and graphs.

However, in a striking result, \citet{stein1956inadmissibility} and \citet{james1961estimation} showed the arithmetic average should not always be the first choice and is inadmissable in even simple settings by todays standards. 
Twenty-seven years later, \citet{gutmann1982stein} proved that this phenomenon cannot occur when the sample spaces are finite.
But even when  the sample mean is admissible, it doesn't close the door to other estimators in all cases.
In many situations where other structural information is hypothesized, for instance a collection of graphs as considered in this paper, other estimators may be preferable.

In complex data settings such as shape data, language data or graph data, we also must take care in how we define the mean.
As with real valued data, one may want to define the mean of a graph to itself be a graph such as for the median graph \citep{jiang2001median}.
However, this may be too restrictive for populations of graphs where there is high variation in which edges appear. 
Instead, we define the mean graph as the weigted adjacency matrix with weights given by the proportion of times the corresponding edge appears in the population. 
This population mean is becoming more and more important both in statistical inference and in various applications like connectomics, social networks, and computational biology.


% Nowadays, people care about information that can be extracted from the networks through different ways in neuroscience.
\citet{ginestet2014hypothesis} proposed a way to test if there is a difference between the networks of two groups of subjects. 
While hypothesis testing is the goal of their work, estimation is a key stepping which may be improved by accounting for underlying structure in the mean matrix. 
Thus improving the estimation procedures for the mean graph is not only important by itself, but also can be applied to help improve other statistical inference procedures.

The element-wise sample mean in many situations, is a reasonable estimator if we consider the general independent edge model (IEM) \cite{bollobas2007phase} without taking any additional structure into account. 
However, it does not perform very well especially when only a small sample size is available.
Intuitively, an estimator incorporating the mean-graph structure is preferable to the entry-wise MLE. 
In general, we don't have any knowledge about this structure so it is hard to take advantage of in practice.



One of the most important structures in graphs is the community structure in which vertices are clustered into groups that share similar connectivity structure. The stochastic blockmodel (SBM) \cite{holland1983stochastic} captures this structural property and is widely used in modeling networks.
More generally, the latent positions model (LPM) \cite{hoff2002latent}, provides a way to parameterize the graph structure by latent positions associated with each vertex. 
Latent position models can capture strong community structure like the stochastic blockmodel, but may also allow for more variance within communities and other structures.
One example of a LPM which captures this middle ground is the random dot product graph (RDPG) \cite{young2007random, nickel2007random} which motivates our estimator. 
In this paper, we analyze our estimator in terms of RDPG specifically.

Using the estimates of the latent positions based on a truncated eigen-decomposition of the adjacency matrix,  in the RDPG setting we consider an estimator for the mean of the collection of graphs which captures the low-rank structure of the RDPG model. In this study, we show via theory, simulations and real data analysis that it frequently outperforms the element-wise MLE, especially in small sample sizes.


% (Future work) Robust estimation, dimension selection, diagonal augmentation, etc.



\section{Models and Estimators}
\label{section:model_estimator}
This work considers the scenario of having $M$ graphs represented as adjacency matrices, $\{A^{(m)}\}$ ($m = 1, \cdots, M$), each having $N$ vertices with known correspondence. The graphs we consider are undirected and unweighted with no self-loops, so each $A^{(m)}$ is a binary symmetric matrix with zeros along the diagonal. An example application of this arises in the field of connectomics, where functional brain imaging data for each subject can be represented as a graph.
Each vertex represent a well defined anatomical region, and an edge between two regions is defined to exist if correlation in activity between the regions surpasses a certain threshold.

We will also assume that the graphs are sampled independently and identically from some distribution.
We consider three nested models for these distributions, the independent edge model, the random dot product model, and the stochastic blockmodel, and two estimators motivated by these models.



\subsection{Independent Edge Model}
The first model we consider is the independent edge model (IEM) with parameter $P \in [0,1]^{N\times N}$ \citep{bollobas2007phase}.
An edge exists between vertex $i$ and vertex $j$ with probability $P_{ij}$ and each edge is present independently of all other edges. 
For this case, the mean graph is the 
For this case, we aim to estimate the mean matrix $P=\Ex[A^{(m)}]$ base on the observed adjacency matrices $A^{(1)},\dotsc,A^{(M)}$.



\subsection{Estimator $\bar{A}$}
Under the IEM, the element-wise sample mean of the adjacency matrices $\bar{A}=\frac{1}{M}\sum_{m=1}^M A^{(m)}$ is the MLE for the mean graph $P$.
It is an unbiased with entry-wise variance $\mathrm{Var}(\bar{A}_{ij}) = P_{ij} (1-P_{ij})/M$. Moreover, $\bar{A}$ is the uniformly minimum-variance unbiased estimator, so it has the smallest variance among all unbiased estimators and enjoys the many asymptotic properties of the MLE as $M\to \infty$.

However, $\bar{A}$ doesn't exploit any graph structure, and sometimes the performance is not very good especially when $M$ is small. 
For example, when $M=1$, $\bar{A}$ is exactly the binary graph we observe, which is an inaccurate estimate for an arbitrary $P$ compared to estimates which exploit underlying structure.



\subsection{Random Dot Product Graph}
In graphs, the adjacencies between vertices always depend on the unobserved properties of the corresponding vertices. For example, in a connectomics setting, the two brain regions with similar properties will have similar connectivity patterns to other regions of the brain.
The latent positions graph model (LPG) proposed by Hoff et. al. (2002) \cite{hoff2002latent} captures such structure, where each vertex is associated with a latent positions that influences the adjacencies for that vertex.
In this model, each vertex $i$ has an associated latent vector $X_i \in \mathbb{R}^d$.
Based on those latent positions, the existence of edges are conditionally independent with probability that only depends on the latent vectors of the incident vertices through a link function. If $d$ is much smaller than the number of vertices $N$ and the link function is known, LPMs are more parsimonious models compared to IEM, requiring only $dN$ parameters rather then $\binom{N}{2}$.

A specific instance of a LPM that we examine in this work is the random dot product graph model (RDPG) \cite{young2007random, nickel2007random} where the link function is the dot product, so the probability of an edge being present between two nodes is the dot product of their latent vectors.

Formally, let $\mathcal{X} \subset \mathbb{R}^d$ be a set such that $x, y \in \mathcal{X}$ implies $\left \langle  x,y \right \rangle \in [0, 1]$.
Let $X_1,\dotsc,X_n\in \mathcal{X}$ and write $X = [X_1|\cdots|X_N]^T \in \mathbb{R}^{N \times d}$.
A random graph $G$ with adjacency matrix $A$ is said to be an RDPG if
\[
	\Pr(A|X) = \prod_{i<j} \left \langle X_i, X_j \right \rangle^{A_{ij}} \left( 1 - \left \langle X_i, X_j \right \rangle \right)^{1 - A_{ij}}.
\]
In the RDPG model, each vertex $i$ is associated with a latent position $X_i$. Conditioned on the latent positions $X$, the edges $A_{ij} \distas{iid} \text{Bern}(\left \langle X_i, X_j \right \rangle)$.
Note that the probability matrix is the outter product of the latent position matrix with itself, $P = X X^T$.
This imposes two properties on $P$, namely that $P$ is positive-semidefinite and $\mathrm{rank}(P)=\mathrm{rank}(X)\leq d$.
%Note that $\mathrm{rank}(P) = \mathrm{rank}(X)$.




\subsection{Low-Rank Estimator $\hat{P}$}

Motivated by the low-rank structure of the RDPG mean matrix, we consider the estimator $\hat{P}$ based on the spectral decomposition of $\bar{A}$ which yields a low rank approximation of $\bar{A}$.
This estiamtor is similar to the estimator proposed by \citet{chatterjee2015matrix} but we analyze its performance more specifically in the random graph setting.

For a given dimension $d$ we consider the estimator $\hat{P}$ defined as the best rank-$d$ positive-semidefinite approximation of $\bar{A}$.
Since the graphs are symmetric, we can compute the eigendecomposition of $\bar{A}$ as $\hat{U} \hat{S} \hat{U}^T + \tilde{U}\tilde{S}\tilde{U}^T$, where $\hat{S}$ is a diagonal matrix with non-increasing entries along the diagonal corresponding to the largest $d$ eigenvalues of $A$ and $\hat{U}$ has columns given by the corresponding eigenvectors.
The $d$-dimensional adjacency spectral embedding (ASE) of $\bar{A}$ is given by $\hat{X}=\hat{U} \hat{S}^{1/2}\in \mathbb{R}^{N \times d}$.
For an RDPG, the rows of $\hat{X}$ are estimates of the latent vectors for each vertex \citep{sussman2014consistent}.
Using the adjacency spectral embedding, we have that $\hat{P} = \hat{X} \hat{X}^T=\hat{U}\hat{S}\hat{U}^T$.

To compute $\hat{P}$, we need to specify what rank $d$ to use and there are various ways of dealing with dimension selection. 
In this paper, we use Zhu and Ghodsi's elbow selection method \cite{zhu2006automatic} and the universal singular value thresholding (USVT) method \cite{chatterjee2015matrix}. 
Details are discussed in Section \ref{section:dim_select}.

Moreover, since the adjacency matrices are hollow, with zeros along the diagonal, there is a missing data problem that leads to inaccuracies if we compute $\hat{P}$ based only on $\bar{A}$. 
To compensate for this issue, we use an iterative method developed by Scheinerman and Tucker \cite{scheinerman2010modeling}. 
Details are discussed in Section \ref{section:diag_aug}.


\begin{algorithm}[H]
\caption{Algorithm to compute $\hat{P}$}
\label{algo:basic}
\begin{algorithmic}[1]
\STATE \textbf{Input:} $A^{(1)}, A^{(2)}, \cdots, A^{(M)}$, each $A^{(m)} \in \{0,1\}^{N \times N}$, $k_max$
\STATE Calculate the sample mean adjacency $\bar{A} = \frac{1}{M}\sum\limits_{m = 1}^M A^{(m)}$;
\STATE Calculate the degree matrix $D^{(0)} = \mathrm{diag}(\bar{A} \bm{1})$;
\STATE Select the dimension $d$  using $\bar{A} + \bar{D}/(n-1)$ (see Section \ref{section:dim_select});
\FOR{$k=0$; $k++$; $k\leq k_{max}$}
\STATE Calculate the rank-$d$ approximation $\hat{U} \hat{S} \hat{U}^T$ for $\bar{A} + D^{(k)}$;
\STATE $D^{(k+1)} \leftarrow \mathrm{diag}(\hat{U} \hat{S} \hat{U}^T)$ (see Section \ref{section:diag_aug}); 
\ENDFOR
\STATE \textbf{Output:} Rank-$d$ approximation $\hat{P}=\hat{U} \hat{S} \hat{U}^T$ of $\bar{A} + D^{(k_{max})}$.
\end{algorithmic}
\end{algorithm}


Algorithm~\ref{algo:basic} gives the steps involved to compute the low-rank estimate $\hat{P}$.
As we will see in the proceeding sections, this procedure will frequently yield improvements in estimation as compared to using the sample mean $\bar{A}$.
While this is unsurprising for random dot product graphs, where we are able to show theoretical results to this effect, we also see this effect for connectome data and more general independent edge graphs.
In the next sections, we explore this estimator in the context of the stochastic blockmodel.

\subsection{Stochastic Block Model as an Random Dot Product Graph}
\label{section:sbm_rdpg}
One of the most important structures for graphs is the community structure in which vertices are clustered into different communities such that vertices of the same community behave similarly. This structural property is captured by the stochastic blockmodel (SBM) \cite{holland1983stochastic}, where each vertex is assigned to a block and the probability that an edge exists between two vertices depends only on their respective block memberships.

The SBM is parameterized by the number of blocks $K$ (generally much less than the number of vertices $N$), the block probability matrix $B \in [0,1]^{K \times K}$, and the vector of block memberships
 % and block proportion vector $\rho \in (0,1)^K$ with $\sum_{k=1}^K \rho_k = 1$. 
$\tau\in\{1,\dotsc,K\}^n$, where for each $i \in [n]$, $\tau_i = k$ means vertex $i$ is a member of block $k$.
% We will assume each vertex is assigned its block independently according to the probability vector $\rho$, so $\Pr[tau_i = k] = \rho_k$. 
Conditioned on $\tau$, each entry of the adjacency matrix $A_{ij}$ is independently sampled from the Bernoulli distribution with parameter $B_{\tau_i,\tau_j}$.

In order to analyze the estimator $\hat{P}$ motivated by RDPG, we move to another representation of SBM based on RDPG. 
Consider a positive semi-definite $K$-block SBM with a rank $d\le K$ block probability matrix $B$, we can always decompose $B$ into $\nu \nu^T$, where $\nu \in \mathbb{R}^{K \times d}$ and each row $\nu_k$ is the shared latent position for all vertices assigned to block $k$. 
For $X \in \mathbb{R}^{N \times d}$ with rows given by $X_i = \nu_{\tau_i}$, we have
\[
	\Pr[A_{ij} = 1|\tau] = B_{\tau_i, \tau_j} = \nu_{\tau_i}^T \nu_{\tau_j}.
\]
In this way, the SBM can be seen as an RDPG where all vertices in the same block will have identical latent positions.

An example SBM is illustrated in Figure \ref{fig:SBM_example}.
We consider a 5-block SBM and plot the corresponding probability matrix and one adjacency matrix generated from it with 200 vertices. From the figure, we can clearly see the structure of 25 blocks in both the probability matrix and the adjacency matrix as a result of 5 different blocks among vertices.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{SBM_P.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{SBM_A.png}
\end{subfigure}
\caption{Example illustrating the stochastic blockmodel. The left figure shows the mean graph $P$ with $K = 5$ blocks and $N=200$ vertices and the right figure shows an adjacency matrix $A$ sampled according to the probabilities from $P$.}
\label{fig:SBM_example}
\end{figure}



\section{Results}

\subsection{Theoretical Results}
\label{section:theoretical_result}
To estimate the mean of a collection of graphs, we consider the two estimators from Section \ref{section:model_estimator}: the entry-wise sample mean $\bar{A}$ and the low-rank $\hat{P}$ motivated by the RDPG.
In this section, we analyze the performance of these two estimators under the SBM by computing the entry-wise relative efficiency (RE), defined as $\mathrm{RE}(\bar{A}_{ij}, \hat{P}_{ij}) = \frac{\mathrm{MSE}(\hat{P}_{ij})}{\mathrm{MSE}(\bar{A}_{ij})}$.
Specifically, we consider the asymptotic relative efficiency as the number of vertices $N\to\infty$ but with the number of graphs $M$ fixed.

For this asymptotic framework, we assume where the block memberships $\tau_i$ are drawn iid from a multinomial distribution with block membership probabilities given by $\rho\in[0,1]^K$.
We will also assume that for a given $N$, the block membership probability are fixed for all graphs.
Denote block probability matrix $B = \nu \nu^T$. 
By definition, the mean of the collection of graphs generated from this SBM is $P$, where $P_{ij} = B_{\tau_i, \tau_j}$. After observing $M$ graphs on $N$ vertices $A^{(1)}, \cdots, A^{(M)}$ sampled independently from the SBM conditioned on $\tau$, we can calculate the two estimators $\bar{A}$ and $\hat{P}$.

\begin{lemma}
\label{lm:VarPhat}
For the above setting, for any $i, j$, if $\mathrm{rank}(B)=K=d$, we have
\[
	\lim_{N \to \infty} N \cdot \mathrm{Var}(\hat{P}_{ij}) =
    \frac{1/\rho_{\tau_i} + 1/\rho_{\tau_j}}{M} P_{ij} (1 - P_{ij}),
\]
and for large enough $N$, we have
\[
	\Ex[(\hat{P}_{ij} - P_{ij})^2] \approx
    \frac{1/\rho_{\tau_i} + 1/\rho_{\tau_j}}{M N} P_{ij}(1-P_{ij}).
\]
\end{lemma}

The proof of this lemma is outlined in Section \ref{section:outline_proof} and is based on results for the variance of the adjacency spectral embedding from \citet{athreya2013limit}. From the result, we can see that the MSE of $\hat{P}_{ij}$ is of order $O(M^{-1}N^{-1})$ approximately.

Moreover, since $\bar{A}_{ij}$ is the sample mean of $M$ independent Bernoulli random variables with parameter $P_{ij}$, we have
\[
	\Ex[(\bar{A}_{ij} - P_{ij})^2] = \frac{P_{ij}(1-P_{ij})}{M}.
\]
This yields the following result.
\begin{theorem}
\label{thm:ARE}
In the same setting as in Lemma \ref{lm:VarPhat}, for any $i$ and $j$, if $\mathrm{rank}(B)=K=d$, the asymptotic relative efficiency (ARE) is 
\[
	\mathrm{ARE}(\bar{A}_{ij}, \hat{P}_{ij}) = \lim_{N \to \infty} \mathrm{RE}(\bar{A}_{ij}, \hat{P}_{ij}) = 0.
\]
and for large enough $N$, we have
\[
	\mathrm{RE}(\bar{A}_{ij}, \hat{P}_{ij}) \approx
    \frac{1/\rho_{\tau_i} + 1/\rho_{\tau_j}}{N}.
\]
\end{theorem}

This theorem indicates that under the SBM, $\hat{P}$ is a much better estimate of the mean of the collection of graphs $P$ than $\bar{A}$. 
From the result, we see that the relative efficiency is of order $O(N^{-1})$ and $N \cdot \mathrm{RE}(\bar{A}_{ij}, \hat{P}_{ij})$ converges to $1/\rho_{\tau_i}+1/\rho_{\tau_j}$ when $N$ goes to infinity.
Also, the ARE does not depend on the number of graphs $M$, so the larger the graphs are, the better $\hat{P}$ is relative to $\bar{A}$, regardless of $M$.

{\color{red} RESULT FOR RDPG}


\subsection{Validation with Simulations}
In this section, we will illustrate the theoretical results from Section 3.1, the relative efficiency between $\bar{A}$ and $\hat{P}$, via Monte Carlo simulation experiments.

%\subsubsection{Simulation Setting}
Here we consider a 2-block SBM parameterized by
\begin{equation*}
B = \begin{bmatrix}
0.42 & 0.2 \\
0.2 & 0.7
\end{bmatrix}
,\qquad \rho = \begin{bmatrix}
0.5 & 0.5
\end{bmatrix}.
\end{equation*}
When calculating $\hat{P}$, we omit the dimension selection step from Algorithm~\ref{algo:basic} and instead using the true dimension $d = \mathrm{rank}(B) = 2$.

%\subsubsection{Simulation Results}
In order to verify the formula for the relative efficiency in Theorem \ref{thm:ARE}, we first sample 1000 Monte Carlo replicates from the above SBM setting with different $N$ and fixed $M$. Based on the result, the scaled relative efficiency $N \cdot \mathrm{RE}(\bar{A}_{ij}, \hat{P}_{ij})$ can be calculated since $P$ is known in this simulation. Moreover, the scaled RE in theory should converge to $1/\rho_{\tau_i}+1/\rho_{\tau_j}$ as $N$ goes to infinity. Since the limit only depends on different blocks rather than different vertices, we combine the result within the same block and define
\[
	\mathrm{RE}_{st}(\bar{A},\hat{P}) = \frac{\sum_{\tau_i=s,\tau_j=t,i \ne j} MSE(\hat{P}_{ij})}{\sum_{\tau_i=s,\tau_j=t,i \ne j} MSE(\bar{A}_{ij})}
\]
for $s,t\in\{1,2\}$.
We plot the scaled relative efficiency $N \cdot \mathrm{RE}_{st}(\bar{A},\hat{P})$ in Figure \ref{fig:RE}. Different types of dashed lines denote the simulated scaled RE associated with different blocks. Solid line represents the theoretical value for scaled RE, which is $1/\rho_t+1/\rho_s=4$ in all cases since $\rho_1=\rho_2=1/2$. From the figure, we see that $N \cdot \mathrm{RE}_{st}(\bar{A}, \hat{P})$ converges to $1/\rho_s + 1/\rho_t$ represented as the solid line, as suggested in Theorem \ref{thm:ARE}. Notice that this means $\mathrm{RE}_{st}(\bar{A}, \hat{P})$ is decreasing at a rate of $O(N^{-1})$.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=1\textwidth]{RE.png}
	\caption{Scaled relative efficiency in average with different $N$ and fixed $M$ based on 1000 Monte Carlo replicates. Different types of dashed lines denote the simulated scaled RE associated with different blocks. Solid line represents the theoretical value for scaled RE. Observe that $N \cdot \mathrm{RE}_{st}(\bar{A}, \hat{P})$ converges to $1/\rho_s + 1/\rho_t = 4$ as expected.}
	\label{fig:RE}
\end{figure}

To illustrate Theorem \ref{thm:ARE} with different $\rho$, we fix $N=500$, $M=100$ and run simulations with $\rho_1$ varying from $0.01$ to $0.50$. Figure \ref{fig:RErho} shows $1/\rho_s + 1/\rho_t$, the limit of scaled RE in theory and the simulated values agree with their corresponding theoretical values very closely. Notice that when $\rho_1 = 0.5$, the scaled RE has value $4.0$, which agrees with the result in Figure \ref{fig:RE}.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Rho.png}
\caption{Simulated results for scaled RE, i.e. $N \cdot \mathrm{RE}_{st}(\bar{A}, \hat{P})$ with $N = 500$ and $M = 100$ of 1000 Monte Carlo replicates while changing $\rho_1$ from 0.1 to 0.9. Scaled relative efficiency in average with different $N$ and fixed $M$ based on 1000 Monte Carlo replicates. Different types of lines denote the simulated values associated with the edges we are averaging over. Notice that when $\rho_1 = 0.5$, the scaled RE has value $4.0$, which agrees with the result in Figure \ref{fig:RE} as expected.}
\label{fig:RErho}
\end{figure}

\subsection{CoRR Brain Graphs: Cross-Validation}

In practice, the graphs may not perfectly follow an RDPG, or even not IEM. But we are still interested in the mean of a collection of graphs. To demonstrate that the estimator $\hat{P}$ is still valid in such cases, we test its performance on DTI data generated with three different atlases (JHU, desikan and CPAC200) available at the Consortium for Reliability and Reproducibility (CoRR) \cite{zuo2014open, gorgolewski2015high}. The dataset contains 454 different brain scans, each of which generates an undirected, unweighted graph with no self-loops. The vertices of the graphs represent different regions in the brain defined by the atlas. For the three atlases we are considering, i.e. JHU, desikan and CPAC200, the number of vertices are 48, 70, 200 respectively. Also, an edge exists between two regions whenever there is at least one white-matter tract connecting the corresponding two parts of the brain. Details of the dataset are provided in Section \ref{section:data}.

By observing 454 graphs generated by the atlas being picked, we use $\hat{P}$ to estimate the mean graph $P$, which is the proportions of the existence of a white-matter tract connecting different parts of the brain. Since $P$ is unknown in practice, we perform a cross-validation study to compare $\bar{A}$ and $\hat{P}$. For each Monte Carlo replicate, we first fix the sample size $M$ and randomly sample $M$ graphs from the total 454 graphs in the dataset. We assure $M$ to be relatively small such that the entry-wise mean of the remaining $(454 - M)$ graphs is a valid approximation of the true probability matrix $P$ we are estimating. Then we can calculate $\bar{A}$ and $\hat{P}$ based on the $M$ samples and compare their performance based on the estimated probability matrix.

We run 1000 simulations on three different atlases with different sample size $M$ and plot the MSE of $\bar{A}$ and $\hat{P}$ in Figure \ref{fig:realdata}. When calculating $\hat{P}$, the dimension $d$ for ASE needs to be selected. For illustrating how the dimension affects the result, we let x-axis to be the dimension $d$ varying from 1 to $N$. Note that this only affects the calculation of $\hat{P}$, that is why the MSE of $\bar{A}$ doesn't change. When $d$ is small, $\hat{P}$ underestimate the dimension and throw away important information, which leads to a relative poor performance. When $d=N$, $\hat{P}$ is equal to $\bar{A}$ since all the information is kept, so that the curve of $\hat{P}$ goes back to $\bar{A}$. In practice, we use algorithms like Zhu and Ghodsi's method or USVT to select the dimension $d$. In the figure, we denote the 3rd elbow of Zhu and Ghodsi by triangle (with largest 95\% confidence interval length to be $3.5$), and denote the USVT with threshold equals 0.7 by square (with largest 95\% confidence interval length to be $0.7$). We can see both algorithms do a good job in dimension selection. When $M$ is small, $\bar{A}$ has large variance which leads to large MSE. Meanwhile, $\hat{P}$ reduces the variance by taking advantages of the graph structure and outperforms $\bar{A}$ with a flexible range of the embedding dimension including what the two algorithms select. With a relatively large $M$, $\bar{A}$ has information to provide a good enough estimation, leaving less space for improvement. As a result, both estimators perform almost perfect when $M$ is large.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{realdata.png}
\caption{Comparison of MSE between $\bar{A}$ (solid line) and $\hat{P}$ (dashed line) for three dataset (JHU, desikan and CPAC200) while embedding the graphs into different dimensions with different size $M$ of the subsamples. The dimension chosen by the 3rd elbow of Zhu and Ghodsi is denoted in triangle (with largest 95\% confidence interval length to be $3.5$), and chosen by USVT with threshold equals 0.7 is denoted in square (with largest 95\% confidence interval length to be $0.7$).  Vertical intervals represent the 95\% confidence interval.  When $M$ is small, $\hat{P}$ outperforms $\bar{A}$ with a flexible range of the embedding dimension including what Zhu and Ghodsi selects.}
\label{fig:realdata}
\end{figure}

For illustration, we randomly take one sample of size $M=5$ based on desikan atlas, and then calculate $\bar{A}$ and $\hat{P}$. The dimension Zhu and Ghodsi's 3rd elbow select is $d=11$. In Figure \ref{fig:Matrix_desikan_m5}, the estimates $\bar{A}$ and $\hat{P}$ as well as the sample mean of 454 graphs (as a close estimate of $P$) are plotted. Since the sample size is small, there are a lot of pairs of vertices with no edges in the 5 observations leading to white spots in $\bar{A}$, especially when the corresponding proportion of the existence of an edge is small. This is compensated in $\hat{P}$ by taking the graph structure into account. In the figure, we see $\hat{P}$ has fewer white spots and look much similar to $P$ compared to $\bar{A}$. Moreover, Figure \ref{fig:Diff_desikan_m5} shows the heat plot of the absolute estimation error $|\bar{A} - P|$ and $|\hat{P}-P|$ respectively. The lower triangular matrix shows the actual absolute difference while the upper triangular matrix only highlights the edges with absolute differences larger than 0.4. There are 18 edges from $\bar{A}$ and 6 edges from $\hat{P}$ being highlighted in the figure, indicating the better performance of $\hat{P}$.

There are lots of places that $\hat{P}$ is better from Figure \ref{fig:Matrix_desikan_m5} and Figure \ref{fig:Diff_desikan_m5} as mentioned above. Here are the actual places in the brain that $\hat{P}$ is better. In Figure \ref{fig:Diff_between_desikan}, we plot the top 5 regions of the brain and top 50 connections between regions with the largest difference $|\bar{A} - P| - |\hat{P} - P|$. Red edges indicate that $\hat{P}$ overestimate $P$ while blue means that $\hat{P}$ underestimate $P$. The edge width is determined by the estimation error. Connections with larger estimation error are represented by thicker lines. This figure shows the regions and connections of the brain where $\hat{P}$ outperforms $\bar{A}$ the most for estimating $P$.

The result demonstrates that $\hat{P}$ gives a better estimate than $\bar{A}$ based on the CoRR dataset with three atlases. 
More importantly, this improvement is insensitive to the embedding dimension we choose as long as we don't underestimate it, which makes our estimator more robust and useful in analyzing real data.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Matrix_desikan_m5.png}
\caption{Comparison between the mean of 454 graphs $P$ and two estimates $\bar{A}$ and $\hat{P}$ derived from a sample of size $M=5$ from desikan dataset while embedding the graphs into dimension $d=11$ selected by the 3rd elbow of ZG method. From the figure, we can see that $\hat{P}$ is a better estimation of $P$ than $\bar{A}$.}
\label{fig:Matrix_desikan_m5}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{Diff2_desikan_m5.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.2\linewidth]{Diff3_desikan_m5.png}
\end{subfigure}
\caption{Heat plot of the absolute estimation error $|\bar{A} - P|$ and $|\hat{P} - P|$ for a sample of size $M=5$ from desikan dataset while embedding the graphs into dimension $d=11$ selected by the 3rd elbow of ZG method. The lower triangular matrix shows the actual absolute difference, while the upper triangular matrix only highlights the edges with absolute differences larger than $0.4$. The fact that 18 edges from $\bar{A}$ and 6 edges from $\hat{P}$ being highlighted shows the better performance of $\hat{P}$.}
\label{fig:Diff_desikan_m5}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{Diff_between_desikan.png}
\caption{Top 5 regions of the brain (vertices in graphs) and top 50 connections between regions (edges in graphs) with largest difference $|\bar{A} - P| - |\hat{P} - P|$. Red edges indicate that $\hat{P}$ overestimate $P$ while blue means that $\hat{P}$ underestimate $P$. The edge width is determined by the estimation error. Connections with larger estimation error are represented by thicker lines. This figure shows the regions and connections of the brain where $\hat{P}$ outperforms $\bar{A}$ the most for estimating $P$.}
\label{fig:Diff_between_desikan}
\end{figure}


%\begin{figure}[!htb]
%\centering
%\includegraphics[width=1\textwidth]{JHU.png}
%\caption{Comparison of MSE between $\bar{A}$ (solid line) and $\hat{P}$ (dashed line) for JHU dataset while embedding the graphs into different dimensions with different size $M$ of the subsamples. The dimension chosen by the 3rd elbow of Zhu and Ghodsi is denoted in triangle, and chosen by USVT with threshold equals 0.7 is denoted in square. Vertical intervals represent the 95\% confidence interval. When $M$ is small, $\hat{P}$ outperforms $\bar{A}$ with a flexible range of the embedding dimension including what Zhu and Ghodsi selects.}
%\label{fig:JHU}
%\end{figure}
%
%\begin{figure}[!htb]
%\centering
%\includegraphics[width=1\textwidth]{desikan.png}
%\caption{Comparison of MSE between $\bar{A}$ (solid line) and $\hat{P}$ (dashed line) for desikan dataset while embedding the graphs into different dimensions with different size $M$ of the subsamples. The dimension chosen by the 3rd elbow of Zhu and Ghodsi is denoted in triangle, and chosen by USVT with threshold equals 0.7 is denoted in square.  Vertical intervals represent the 95\% confidence interval.  When $M$ is small, $\hat{P}$ outperforms $\bar{A}$ with a flexible range of the embedding dimension including what Zhu and Ghodsi selects.}
%\label{fig:desikan}
%\end{figure}
%
%\begin{figure}[!htb]
%\centering
%\includegraphics[width=1\textwidth]{CPAC200.png}
%\caption{Comparison of MSE between $\bar{A}$ (solid line) and $\hat{P}$ (dashed line) for CPAC200 dataset while embedding the graphs into different dimensions with different size $M$ of the subsamples. The dimension chosen by the 3rd elbow of Zhu and Ghodsi is denoted in triangle, and chosen by USVT with threshold equals 0.7 is denoted in square.  Vertical intervals represent the 95\% confidence interval.  When $M$ is small, $\hat{P}$ outperforms $\bar{A}$ with a flexible range of the embedding dimension including what Zhu and Ghodsi selects.}
%\label{fig:CPAC200}
%\end{figure}
%
%\begin{figure}
%\centering
%\begin{subfigure}{.33\textwidth}
%  \centering
%  \includegraphics[width=1.2\linewidth]{P_JHU.png}
%\end{subfigure}%
%\begin{subfigure}{.33\textwidth}
%  \centering
%  \includegraphics[width=1.2\linewidth]{Abar_JHU_m5.png}
%\end{subfigure}
%\begin{subfigure}{.33\textwidth}
%  \centering
%  \includegraphics[width=1.2\linewidth]{Phat_JHU_m5.png}
%\end{subfigure}
%\caption{Comparison between the mean of 454 graphs $P$ and two estimates $\bar{A}$ and $\hat{P}$ derived from a sample of size $M=5$ from JHU dataset while embedding the graphs into dimension $d=15$ selected by the 3rd elbow of ZG method.}
%\label{fig:adj_JHU_m5}
%\end{figure}
%
%\begin{figure}[!htb]
%\centering
%\includegraphics[width=1\textwidth]{Vertex_Diff_Phat_desikan.png}
%\caption{Top 5 regions of the brain (vertices in graphs) with largest absolute difference $|\hat{P} - P|$.}
%\label{fig:Vertex_Diff_Phat_desikan}
%\end{figure}
%
%\begin{figure}[!htb]
%\centering
%\includegraphics[width=1\textwidth]{Edge_Diff_Phat_desikan.png}
%\caption{Top 1\% (49) connections between regions (edges in graphs) with largest absolute difference $|\hat{P} - P|$.}
%\label{fig:Vertex_Diff_Phat_desikan}
%\end{figure}


\subsection{Simulation under the Full Rank Independent Edge Model}

While the theory we have is based on the low rank assumption implied by the RDPG, $\hat{P}$ sometimes wins the bias-variance tradeoff even when the graphs have full rank structure. To illustrate this point, instead of the low rank SBM, we run simulations under the full rank independent edge model with the probability matrix $P$, defined as the sample mean of the 454 graphs in the desikan dataset.

Figure \ref{fig:sim_desikan} compare the MSE between $\bar{A}$ (solid line) and $\hat{P}$ (dashed line) for simulated data based on the full rank probability matrix $P$ as the sample mean in desikan dataset while embedding the graphs into different dimensions with different size $M$ of the subsamples. Vertical intervals represent the 95\% confidence interval. We see similar results as in the SBM setting. When $M$ is small, $\hat{P}$ outperforms $\bar{A}$ with a flexible range of the embedding dimension including what Zhu and Ghodsi selects. When $M$ is large enough, both estimators perform almost perfect.

So $\hat{P}$ does a good job even when the low rank assumption of the model is violated. This simulation shows again the robustness of $\hat{P}$.


\begin{figure}[!htb]
\centering
\includegraphics[width=1\textwidth]{sim_desikan.png}
\caption{Comparison of MSE between $\bar{A}$ (solid line) and $\hat{P}$ (dashed line) for simulated data based on the full rank probability matrix $P$ as the sample mean in desikan dataset while embedding the graphs into different dimensions with different size $M$ of the subsamples. Vertical intervals represent the 95\% confidence interval. When $M$ is small, $\hat{P}$ outperforms $\bar{A}$ with a flexible range of the embedding dimension including what Zhu and Ghodsi selects.}
\label{fig:sim_desikan}
\end{figure}



\section{Discussion}

\subsection{Summary}
In this paper, we propose a better way to estimate the mean of a collection of graphs.Motivated by RDPG, our methodology take advantage of the low-rank structure of the graphs by applying ASE to the entry-wise MLE. We then give a closed form for the asymptotical relative efficiency between the entry-wise MLE $\bar{A}$ and our estimator $\hat{P}$, which theoretically proves that $\hat{P}$ has smaller MSE when $N$ is sufficiently large. These results are demonstrate by various simulations. Moreover, our estimator also outperforms the entry-wise MLE in the CoRR brain graphs and in the full-rank simulation, which shows that $\hat{P}$ still performs well even when the low-rank assumption is violated. It demonstrate that $\hat{P}$ is robust and can be applied in practice.


\subsection{Future Work}
In this paper, we assume that the adjacency matrix is observed without contamination. However, generally there will be noise in practice. With contaminations, robust estimators like ML$q$E is preferred. If an estimator can not only inherit robustness from the robust estimators but also has small variance by taking advantage of the low rank structure of the graphs, it will be very useful.

Meanwhile, estimating the rank of the graph structure accurately will certainly help improve the performance of the estimator $\hat{P}$. Now we are using Zhu and Ghodsi's method and USVT, but there is still a lot of space for improvement, especially in this particular case.







\section{Methods}

\subsection{Choosing Dimension}
\label{section:dim_select}
Often in dimensionality reduction techniques, the choice for dimension $d$, relies on visually analyzing a plot of the ordered eigenvalues, looking for a ``gap'' or ``elbow'' in the scree-plot. Zhu and Ghodsi \cite{zhu2006automatic} present an automated method for finding this gap in the scree-plot that takes only the ordered eigenvalues as an input. In order to prevent underestimating $d$, which is much more harmful than overestimating, we use the 3rd elbow in the experiments performed in this work.

Universal Singular Value Thresholding (USVT) \cite{chatterjee2015matrix} is a simple estimation procedure proposed by Chatterjee that works for any matrix that has ``a little bit of structure''. Basically in our setting, it selects the dimension $d$ as the number singular values that are greater than a constant $c$ times $\sqrt{N/M}$. In the simulation, we set $c = 0.7$.



\subsection{Graph Diagonal Augmentation}
\label{section:diag_aug}
The graphs examined in this work are hollow, in that there are no self-loops and thus the diagonal entries of the adjacency matrix are 0. This leads to a bias in the calculation of the eigenvectors. To compensate such bias, we use an iterative method developed by Scheinerman and Tucker \cite{scheinerman2010modeling} to augment the diagonal before ASE. In the experiments, we are using 1 iteration of Scheinerman's method to do the diagonal augmentation.



\subsection{Dataset Description}
\label{section:data}
The original dataset is from the Emotion and Creativity One Year Retest Dataset provided by Qiu, Zhang and Wei from Southwest University available at the Consortium for Reliability and Reproducibility (CoRR) \cite{zuo2014open, gorgolewski2015high}. It is comprised of 235 subjects, all of whom were college students. Each subject underwent two sessions of anatomical, resting state DTI scans, spaced one year apart. Due to the incomplete data, the true number of scans is 454.

When deriving MR connectomes, the NeuroData team parcellate the brain into groups of nodes as defined by anatomical atlases \cite{neurodata, kiar2016graph}. The atlases are defined either physiologically or structurally by neuroanatomists (Desikan and JHU), or are generated using a segmentation algorithm looking for certain features or groupings (CPAC200).

The graphs we are using are processed by NeuroData team from DTI data of the original dataset generated with different atlases (desikan, JHU and CPAC200), each containing different region/node definitions. The graphs are undirected, unweighted and with no self-loops. An edge exists between two regions when there is at least one white-matter tract connecting the corresponding two parts of the brain.


\subsection{Outline for the Proof of the Theorems}
\label{section:outline_proof}
Here we provide an outline of the proof for the MSE($\hat{P}$) result presented in Section \ref{section:theoretical_result}.
	
When comparing two estimators, the first thing we need to consider is consistency.
It is easy to see that $\bar{A}$ is unbiased as an estimate of $P$. Moreover, since two latent positions are conditionally asymptotically independent by extended version of Theorem 1 in Athreya et al. (2013) \cite{athreya2013limit}, we know $\hat{P}$ is consistent, as well as $\bar{A}$.

Thus the relative efficiency between $\hat{P}$ and $\bar{A}$, which is equivalent to the ratio of mean square errors in this case, is a good indicate in comparison.

Since $\hat{P}_{ij} = \hat{X}_i^T \hat{X}_j$ is a noisy version of the dot product of $\nu_s^T \nu_t$, by Equation 5 in Brown and Rutemiller (1977) \cite{brown1977means}, combined with asymptotic independence between $\hat{X}_i$ and $\hat{X}_j$, and the covariance matrices given by extended version of Theorem 1 in Athreya et al. (2013) \cite{athreya2013limit}, we have the variance of $\hat{P}_{ij}$ converges to $\left( 1/\rho_{\tau_i} + 1/\rho_{\tau_j} \right) P_{ij} (1-P_{ij})/(N \cdot M)$ as $N \rightarrow \infty$. Since the variance of $\bar{A}_{ij}$ is $P_{ij} (1-P_{ij})/M$, the relative efficiency between $\hat{P}_{ij}$ and $\bar{A}_{ij}$ is approximately $(\rho_{\tau_i}^{-1} + \rho_{\tau_j}^{-1})/N$ when $N$ is sufficiently large.
	
The (relative) full proof is provided here: \\
https://www.overleaf.com/2776898cydwhv. Feel free to edit it.

\bibliography{Bib}
% \bibliographystyle{plain}


\end{document}