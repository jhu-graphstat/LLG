\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{trunk1979problem}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{stein1956inadmissibility}
\citation{james1961estimation}
\citation{gutmann1982stein}
\citation{jiang2001median}
\citation{ginestet2014hypothesis}
\citation{bollobas2007phase}
\citation{holland1983stochastic}
\citation{hoff2002latent}
\citation{young2007random,nickel2007random}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Heat maps of the population mean, the sample mean, and the estimator $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$.} These heat maps indicate the population mean for the $454$ graphs (left), sample mean for the 5 sampled graphs (center), and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ for the 5 sampled graphs with dimension $d=11$ selected using the Zhu and Ghodsi method (right). Details about how to construct $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ are discussed in Section\nobreakspace  {}\ref  {sec:phat}. Darker pixels indicate a lower probability of an edge between the given vertices. Note that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ appears to better estimate the true probability matrix $P$, especially for edges within the two hemispheres, in the upper left and corresponding lower right block. And by calculating the mean squared error based on this sample, we can also see that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ (with mean squared error equal 0.017) outperforms $\mathaccentV {bar}016{A}$ (with mean squared error equal 0.027). \relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Matrix_desikan_m5}{{1}{3}{{\bf Heat maps of the population mean, the sample mean, and the estimator $\hat {P}$.} These heat maps indicate the population mean for the $454$ graphs (left), sample mean for the 5 sampled graphs (center), and $\hat {P}$ for the 5 sampled graphs with dimension $d=11$ selected using the Zhu and Ghodsi method (right). Details about how to construct $\hat {P}$ are discussed in Section~\ref {sec:phat}. Darker pixels indicate a lower probability of an edge between the given vertices. Note that $\hat {P}$ appears to better estimate the true probability matrix $P$, especially for edges within the two hemispheres, in the upper left and corresponding lower right block. And by calculating the mean squared error based on this sample, we can also see that $\hat {P}$ (with mean squared error equal 0.017) outperforms $\bar {A}$ (with mean squared error equal 0.027). \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Models}{4}{section.2}}
\newlabel{section:model}{{2}{4}{Models}{section.2}{}}
\citation{bollobas2007phase}
\citation{hoff2002latent}
\citation{young2007random,nickel2007random}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Independent Edge Model}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Random Dot Product Graph}{5}{subsection.2.2}}
\citation{holland1983stochastic}
\citation{ambroise2012new,wolfe2013nonparametric,choi2012stochastic,picard2009deciphering,zanghi2008fast,zanghi2010clustering,pavlovic2014stochastic,daudin2008mixture}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stochastic Blockmodel as a Random Dot Product Graph}{6}{subsection.2.3}}
\newlabel{section:sbm_rdpg}{{2.3}{6}{Stochastic Blockmodel as a Random Dot Product Graph}{subsection.2.3}{}}
\citation{chatterjee2015matrix}
\@writefile{toc}{\contentsline {section}{\numberline {3}Estimators}{7}{section.3}}
\newlabel{sec:estimator}{{3}{7}{Estimators}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Element-wise sample mean}{7}{subsection.3.1}}
\newlabel{sec:abar}{{3.1}{7}{Element-wise sample mean}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Low-Rank Estimator}{7}{subsection.3.2}}
\newlabel{sec:phat}{{3.2}{7}{Low-Rank Estimator}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  Example illustrating the stochastic blockmodel.} The top left figure shows the mean graph $P$ with $K = 5$ blocks and $N=200$ vertices and the top right figure shows an adjacency matrix $A$ sampled according to the probabilities from $P$. While $A$ is a noisy version of $P$, much of the structure of $P$ is preserved in $A$, a property we will exploit in our estimation procedure. Based on three graphs sampled independently and identically according to the probability matrix $P$, we construct the element-wise mean $\mathaccentV {bar}016{A}$, shown in the lower right panel (see Section\nobreakspace  {}\ref  {sec:abar}). Finally, by taking a rank-5 approximation of $\mathaccentV {bar}016{A}$ and thresholding the values to be between $0$ and $1$, we construct our proposed estimate $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$, shown in the lower left panel (see Section\nobreakspace  {}\ref  {sec:phat}). By visual inspection, it is clear that the low-rank estimate $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ more closely approximates the probability matrix $P$ as compared to $\mathaccentV {bar}016{A}$. \relax }}{8}{figure.caption.2}}
\newlabel{fig:SBM_example}{{2}{8}{{\bf Example illustrating the stochastic blockmodel.} The top left figure shows the mean graph $P$ with $K = 5$ blocks and $N=200$ vertices and the top right figure shows an adjacency matrix $A$ sampled according to the probabilities from $P$. While $A$ is a noisy version of $P$, much of the structure of $P$ is preserved in $A$, a property we will exploit in our estimation procedure. Based on three graphs sampled independently and identically according to the probability matrix $P$, we construct the element-wise mean $\bar {A}$, shown in the lower right panel (see Section~\ref {sec:abar}). Finally, by taking a rank-5 approximation of $\bar {A}$ and thresholding the values to be between $0$ and $1$, we construct our proposed estimate $\hat {P}$, shown in the lower left panel (see Section~\ref {sec:phat}). By visual inspection, it is clear that the low-rank estimate $\hat {P}$ more closely approximates the probability matrix $P$ as compared to $\bar {A}$. \relax }{figure.caption.2}{}}
\citation{sussman2014consistent}
\citation{zhu2006automatic}
\citation{chatterjee2015matrix}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm to compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$\relax }}{9}{algorithm.1}}
\newlabel{algo:basic}{{1}{9}{Algorithm to compute $\hat {P}$\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Algorithm to compute the rank-$d$ approximation of a matrix.\relax }}{9}{algorithm.2}}
\newlabel{algo:lowrank}{{2}{9}{Algorithm to compute the rank-$d$ approximation of a matrix.\relax }{algorithm.2}{}}
\citation{scheinerman2010modeling}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{10}{section.4}}
\newlabel{sec:result}{{4}{10}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Asymptotic Theory}{10}{subsection.4.1}}
\newlabel{section:theoretical_result}{{4.1}{10}{Asymptotic Theory}{subsection.4.1}{}}
\citation{athreya2013limit}
\newlabel{lm:VarPhat}{{4.1}{11}{}{fact.4.1}{}}
\newlabel{thm:ARE}{{4.2}{11}{}{fact.4.2}{}}
\newlabel{eq:approx_re}{{1}{11}{}{equation.4.1}{}}
\newlabel{eq:sbm_are}{{4.2}{11}{}{equation.4.1}{}}
\newlabel{eq:sim_setting}{{2}{12}{Asymptotic Theory}{equation.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Asymptotic scaled relative efficiency $N\cdot \mathrm  {RE}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$ in a 2-block SBM.} For each distinct pair of edge probabilities in a 2-block SBM specified in Eq.\nobreakspace  {}\ref  {eq:sim_setting}, the scaled relative efficiency only depends on the proportion of vertices in each block. We show the scaled asymptotic relative efficiency as $\rho _1$ changes from $0,1$ for pairs of vertices where either both are in block one or one is in block one and one is in block two. These curves all intersect at a scaled relative efficiency of 4 when $\rho _1=1/2=\rho _2$. Improvements using low-rank methods are greater for larger blocks, such as for $B_{11}$ when $\rho _1$ is close to 1, while the improvements are smaller for block pairs with relatively few vertex pairs such as $B_{11}$ when $\rho _1$ is small and $B_{12}$ when $\rho _1$ is near 0 or 1 . Note that the curve for $B_{22}$ would be the same as that for $B_{11}$ but reflected around the vertical line when $\rho _1=1/2$. Overall, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ performs best for large blocks while the improvements may be very minor for blocks with only a few vertices. \relax }}{13}{figure.caption.3}}
\newlabel{fig:RErho}{{3}{13}{{\bf Asymptotic scaled relative efficiency $N\cdot \mathrm {RE}(\bar {A},\hat {P})$ in a 2-block SBM.} For each distinct pair of edge probabilities in a 2-block SBM specified in Eq.~\ref {eq:sim_setting}, the scaled relative efficiency only depends on the proportion of vertices in each block. We show the scaled asymptotic relative efficiency as $\rho _1$ changes from $0,1$ for pairs of vertices where either both are in block one or one is in block one and one is in block two. These curves all intersect at a scaled relative efficiency of 4 when $\rho _1=1/2=\rho _2$. Improvements using low-rank methods are greater for larger blocks, such as for $B_{11}$ when $\rho _1$ is close to 1, while the improvements are smaller for block pairs with relatively few vertex pairs such as $B_{11}$ when $\rho _1$ is small and $B_{12}$ when $\rho _1$ is near 0 or 1 . Note that the curve for $B_{22}$ would be the same as that for $B_{11}$ but reflected around the vertical line when $\rho _1=1/2$. Overall, $\hat {P}$ performs best for large blocks while the improvements may be very minor for blocks with only a few vertices. \relax }{figure.caption.3}{}}
\newlabel{remark:low_rank}{{4.4}{14}{}{fact.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Finite Sample Toy Model Simulations}{14}{subsection.4.2}}
\newlabel{sec:sbm_sim}{{4.2}{14}{Finite Sample Toy Model Simulations}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Finite sample relative efficiency based on simulations. } The top panel shows the estimated relative efficiency $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {RE}$}\mathaccent "0362{\mathrm  {RE}}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$ as a function of $N$ for fixed $M=100$ based on simulations of an SBM. For each value of $N$, we used 1000 Monte Carlo replicates of the SBM from Section\nobreakspace  {}\ref  {sec:sbm_sim} to estimate the RE. Each curve corresponds to an average across vertex pairs corresponding to the three distinct block probabilities $B_{11}$, $B_{12}$, and $B_{22}$ in the two-block SBM. Recall that values below 1 indicate that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ is performing better than $\mathaccentV {bar}016{A}$. The relative efficiencies are all very close so the lines are indistinguishable.   To distinguish the three curves, the bottom panel shows the corresponding scaled relative efficiencies, $N\cdot \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {RE}$}\mathaccent "0362{\mathrm  {RE}}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$. The solid horizontal line indicates the theoretical asymptotic scaled relative which is $1/\rho _s+1/\rho _t=4$, since $\rho _1=\rho _2=4$. All the curves converge quickly to this theoretical limit. \relax }}{15}{figure.caption.4}}
\newlabel{fig:RE}{{4}{15}{{\bf Finite sample relative efficiency based on simulations. } The top panel shows the estimated relative efficiency $\hat {\mathrm {RE}}(\bar {A},\hat {P})$ as a function of $N$ for fixed $M=100$ based on simulations of an SBM. For each value of $N$, we used 1000 Monte Carlo replicates of the SBM from Section~\ref {sec:sbm_sim} to estimate the RE. Each curve corresponds to an average across vertex pairs corresponding to the three distinct block probabilities $B_{11}$, $B_{12}$, and $B_{22}$ in the two-block SBM. Recall that values below 1 indicate that $\hat {P}$ is performing better than $\bar {A}$. The relative efficiencies are all very close so the lines are indistinguishable. \\ To distinguish the three curves, the bottom panel shows the corresponding scaled relative efficiencies, $N\cdot \hat {\mathrm {RE}}(\bar {A},\hat {P})$. The solid horizontal line indicates the theoretical asymptotic scaled relative which is $1/\rho _s+1/\rho _t=4$, since $\rho _1=\rho _2=4$. All the curves converge quickly to this theoretical limit. \relax }{figure.caption.4}{}}
\citation{zuo2014open,gorgolewski2015high}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}CoRR Brain Graphs: Cross-Validation}{16}{subsection.4.3}}
\newlabel{sec:corr_data}{{4.3}{16}{CoRR Brain Graphs: Cross-Validation}{subsection.4.3}{}}
\citation{gray2013migraine}
\citation{kiar2016m2g}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\bf  Screeplot of the population mean.} These screeplots show the eigenvalues of the mean graph of all 454 graphs with diagonal augmentation in decreasing algebraic order for three atlases. Many eigenvalues are around zero, which lead to a quasi low-rank structure. \relax }}{17}{figure.caption.5}}
\newlabel{fig:screeplot}{{5}{17}{{\bf Screeplot of the population mean.} These screeplots show the eigenvalues of the mean graph of all 454 graphs with diagonal augmentation in decreasing algebraic order for three atlases. Many eigenvalues are around zero, which lead to a quasi low-rank structure. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\bf  Histogram of the population mean.} These figures show the histograms of the eigenvalues of the mean graph of all 454 graphs with diagonal augmentation. Many eigenvalues are around zero, which lead to a quasi low-rank structure. \relax }}{18}{figure.caption.6}}
\newlabel{fig:histogram}{{6}{18}{{\bf Histogram of the population mean.} These figures show the histograms of the eigenvalues of the mean graph of all 454 graphs with diagonal augmentation. Many eigenvalues are around zero, which lead to a quasi low-rank structure. \relax }{figure.caption.6}{}}
\citation{zhu2006automatic}
\citation{chatterjee2015matrix}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\bf  Comparison of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ and $\mathaccentV {bar}016{A}$ for three atlases at four sample sizes for the CoRR data.} These plots show the mean squared error for $\mathaccentV {bar}016{A}$ (solid line) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ (dashed line) for three dataset (JHU, Desikan, and CPAC200) while embedding the graphs into different dimensions and with different sample sizes $M$. The average dimensions chosen by the 3rd elbow of Zhu and Ghodsi is denoted by a triangle and those chosen by USVT with threshold equaling 0.7 is denoted by a square. Vertical intervals, visible mainly in the $N=48,70$ and $M=1$ plots, represent the 95\% confidence interval for the mean squared errors. When $M$ is small, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ outperforms $\mathaccentV {bar}016{A}$ with a flexible range of the embedding dimension including the average of the dimensions selected by Zhu and Ghodsi and USVT.\relax }}{20}{figure.caption.7}}
\newlabel{fig:realdata}{{7}{20}{{\bf Comparison of $\hat {\mathrm {MSE}}$ of $\hat {P}$ and $\bar {A}$ for three atlases at four sample sizes for the CoRR data.} These plots show the mean squared error for $\bar {A}$ (solid line) and $\hat {P}$ (dashed line) for three dataset (JHU, Desikan, and CPAC200) while embedding the graphs into different dimensions and with different sample sizes $M$. The average dimensions chosen by the 3rd elbow of Zhu and Ghodsi is denoted by a triangle and those chosen by USVT with threshold equaling 0.7 is denoted by a square. Vertical intervals, visible mainly in the $N=48,70$ and $M=1$ plots, represent the 95\% confidence interval for the mean squared errors. When $M$ is small, $\hat {P}$ outperforms $\bar {A}$ with a flexible range of the embedding dimension including the average of the dimensions selected by Zhu and Ghodsi and USVT.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\bf  Difference between $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ of $\mathaccentV {bar}016{A}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ for three atlases at four sample sizes for the CoRR data.} To give concrete numbers of the experiment results displayed in Fig.\nobreakspace  {}\ref  {fig:realdata}, we list $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ of $\mathaccentV {bar}016{A}$ minus $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ in the table. A positive number means $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ outperforms $\mathaccentV {bar}016{A}$. We compared different sample sizes $M$ and different dimension selection procedures, ZG and USVT. Confidence intervals all had lengths less than $0.015$, and hence we omitted them for clarity. Overall, the relative efficiencies are greater for smaller sample sizes $M$ and larger number of vertices $N$. \relax }}{21}{table.caption.8}}
\newlabel{tab:corr_mse}{{1}{21}{{\bf Difference between $\hat {\mathrm {MSE}}$ of $\bar {A}$ and $\hat {\mathrm {MSE}}$ of $\hat {P}$ for three atlases at four sample sizes for the CoRR data.} To give concrete numbers of the experiment results displayed in Fig.~\ref {fig:realdata}, we list $\hat {\mathrm {MSE}}$ of $\bar {A}$ minus $\hat {\mathrm {MSE}}$ of $\hat {P}$ in the table. A positive number means $\hat {P}$ outperforms $\bar {A}$. We compared different sample sizes $M$ and different dimension selection procedures, ZG and USVT. Confidence intervals all had lengths less than $0.015$, and hence we omitted them for clarity. Overall, the relative efficiencies are greater for smaller sample sizes $M$ and larger number of vertices $N$. \relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {\bf  Relative efficiencies of $\mathaccentV {bar}016{A}$ versus $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ for the CoRR data set.} For each atlas, JHU, Desikan, and CPAC 200, we sampled graphs which we used to compute $\mathaccentV {bar}016{A}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$. We compared different sample sizes $M$ and different dimension selection procedures, ZG and USVT. For each of the two methods for computing $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$, we estimated their relative efficiencies with respect to the sample mean $\mathaccentV {bar}016{A}$. For $M = 1, 5, 10$, all confidence intervals had lengths less than $0.008$, while for $M=50$ the confidence intervals had length less than $0.046$, and hence we omitted them for clarity. Overall, the relative efficiencies are greater for smaller sample sizes $M$ and larger number of vertices $N$. Although for $M=50$ the RE is large, from Table\nobreakspace  {}\ref  {tab:corr_mse} we can see this is due to the small MSE of $\mathaccentV {bar}016{A}$ as the denominator. In this case the actual difference of the performance between $\mathaccentV {bar}016{A}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ is quite small. \relax }}{22}{table.caption.9}}
\newlabel{tab:corr_re}{{2}{22}{{\bf Relative efficiencies of $\bar {A}$ versus $\hat {P}$ for the CoRR data set.} For each atlas, JHU, Desikan, and CPAC 200, we sampled graphs which we used to compute $\bar {A}$ and $\hat {P}$. We compared different sample sizes $M$ and different dimension selection procedures, ZG and USVT. For each of the two methods for computing $\hat {P}$, we estimated their relative efficiencies with respect to the sample mean $\bar {A}$. For $M = 1, 5, 10$, all confidence intervals had lengths less than $0.008$, while for $M=50$ the confidence intervals had length less than $0.046$, and hence we omitted them for clarity. Overall, the relative efficiencies are greater for smaller sample sizes $M$ and larger number of vertices $N$. Although for $M=50$ the RE is large, from Table~\ref {tab:corr_mse} we can see this is due to the small MSE of $\bar {A}$ as the denominator. In this case the actual difference of the performance between $\bar {A}$ and $\hat {P}$ is quite small. \relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Synthetic Data Analysis for Full Rank IEM}{22}{subsection.4.4}}
\newlabel{sec:sim_iem}{{4.4}{22}{Synthetic Data Analysis for Full Rank IEM}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\bf  Heat plot of absolute estimation error for $\bm  {\mathaccentV {bar}016{A}}$ and $\bm  {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}}$ (lower triangle) and absolute errors above 0.4 (upper triangle).} These heat plots show the absolute estimation error $|\mathaccentV {bar}016{A} - P|$, $|\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P} - P|$ and $|\mathaccentV {bar}016{A} - \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}|$ for a sample of size $M=5$ from the Desikan dataset. The embedding dimension for $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ is $d=11$ selected by the 3rd elbow of the ZG method. The lower triangular matrix shows the actual absolute difference, while the upper triangular matrix only highlights the edges with absolute differences larger than $0.4$. The fact that 18 edges from $\mathaccentV {bar}016{A}$ are highlighted and only six edges from $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ are highlighted indicates that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ has fewer large outliers compared to $\mathaccentV {bar}016{A}$. \relax }}{23}{figure.caption.10}}
\newlabel{fig:Diff_desikan_m5}{{8}{23}{{\bf Heat plot of absolute estimation error for $\bm {\bar {A}}$ and $\bm {\hat {P}}$ (lower triangle) and absolute errors above 0.4 (upper triangle).} These heat plots show the absolute estimation error $|\bar {A} - P|$, $|\hat {P} - P|$ and $|\bar {A} - \hat {P}|$ for a sample of size $M=5$ from the Desikan dataset. The embedding dimension for $\hat {P}$ is $d=11$ selected by the 3rd elbow of the ZG method. The lower triangular matrix shows the actual absolute difference, while the upper triangular matrix only highlights the edges with absolute differences larger than $0.4$. The fact that 18 edges from $\bar {A}$ are highlighted and only six edges from $\hat {P}$ are highlighted indicates that $\hat {P}$ has fewer large outliers compared to $\bar {A}$. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces {\bf  Top 5 regions of the brain (vertices in graphs) and top 50 connections between regions (edges in graphs) with the largest differences $\bm  {|\mathaccentV {bar}016{A}_{ij} - P_{ij}| - |\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}_{ij} - P_{ij}|}$.} Red edges indicate that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ overestimate $P$ while blue means that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ underestimates $P$. The edge width is determined by the estimation error. Connections with larger estimation error are represented by thicker lines. This figure shows the regions and connections of the brain where $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ outperforms $\mathaccentV {bar}016{A}$ the most for estimating $P$.\relax }}{23}{figure.caption.11}}
\newlabel{fig:Diff_between_desikan}{{9}{23}{{\bf Top 5 regions of the brain (vertices in graphs) and top 50 connections between regions (edges in graphs) with the largest differences $\bm {|\bar {A}_{ij} - P_{ij}| - |\hat {P}_{ij} - P_{ij}|}$.} Red edges indicate that $\hat {P}$ overestimate $P$ while blue means that $\hat {P}$ underestimates $P$. The edge width is determined by the estimation error. Connections with larger estimation error are represented by thicker lines. This figure shows the regions and connections of the brain where $\hat {P}$ outperforms $\bar {A}$ the most for estimating $P$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces {\bf  Comparison of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ and $\mathaccentV {bar}016{A}$ for synthetic data analysis.} As in Fig.\nobreakspace  {}\ref  {fig:RE}, this figure shows $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ for $\mathaccentV {bar}016{A}$ (solid line) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ (dashed line) for simulated data with different sample sizes $M$ based on the sample mean for the Desikan dataset. Again, the average of dimensions selected by the USVT method (square) and the ZG method (triangle) tend to nearly approximate the optimal dimension. Overall, we see that the structure of these plots well approximates the structure for the real data indicating that performance for the independent edge model will tend to translate in structure to non-independent edge scenarios. On the other hand, the relative efficiency $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {RE}$}\mathaccent "0362{\mathrm  {RE}}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$ is lower for this synthetic data analysis than for the CoRR data.\relax }}{24}{figure.caption.12}}
\newlabel{fig:sim_desikan}{{10}{24}{{\bf Comparison of $\hat {P}$ and $\bar {A}$ for synthetic data analysis.} As in Fig.~\ref {fig:RE}, this figure shows $\hat {\mathrm {MSE}}$ for $\bar {A}$ (solid line) and $\hat {P}$ (dashed line) for simulated data with different sample sizes $M$ based on the sample mean for the Desikan dataset. Again, the average of dimensions selected by the USVT method (square) and the ZG method (triangle) tend to nearly approximate the optimal dimension. Overall, we see that the structure of these plots well approximates the structure for the real data indicating that performance for the independent edge model will tend to translate in structure to non-independent edge scenarios. On the other hand, the relative efficiency $\hat {\mathrm {RE}}(\bar {A},\hat {P})$ is lower for this synthetic data analysis than for the CoRR data.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{24}{section.5}}
\newlabel{sec:discussion}{{5}{24}{Discussion}{section.5}{}}
\citation{durante2014nonparametric}
\citation{huber2009robust,qin2013maximum}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces {\bf  Heat map of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0362{X}$.} Heat map of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0362{X}$ with each row to be the estimated latent position for the corresponding vertex. From the second column, we can see a clear distinction of the left and right hemisphere as conveyed in the second dimension.\relax }}{26}{figure.caption.13}}
\newlabel{fig:eigenvector}{{11}{26}{{\bf Heat map of $\hat {X}$.} Heat map of $\hat {X}$ with each row to be the estimated latent position for the corresponding vertex. From the second column, we can see a clear distinction of the left and right hemisphere as conveyed in the second dimension.\relax }{figure.caption.13}{}}
\citation{zhu2006automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces {\bf  Brain colored by the 2nd dimension of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0362{X}$.} We plot the brain using the 2nd dimension of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0362{X}$. From the figure, we can see a clear distinction of the left and right hemisphere as conveyed in the second dimension.\relax }}{27}{figure.caption.14}}
\newlabel{fig:eigenvector_brain}{{12}{27}{{\bf Brain colored by the 2nd dimension of $\hat {X}$.} We plot the brain using the 2nd dimension of $\hat {X}$. From the figure, we can see a clear distinction of the left and right hemisphere as conveyed in the second dimension.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Methods}{27}{section.6}}
\newlabel{sec:method}{{6}{27}{Methods}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Choosing Dimension}{27}{subsection.6.1}}
\newlabel{section:dim_select}{{6.1}{27}{Choosing Dimension}{subsection.6.1}{}}
\citation{chatterjee2015matrix}
\citation{marchette2011vertex}
\citation{scheinerman2010modeling}
\citation{zuo2014open,gorgolewski2015high}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Graph Diagonal Augmentation}{28}{subsection.6.2}}
\newlabel{section:diag_aug}{{6.2}{28}{Graph Diagonal Augmentation}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Dataset Description}{28}{subsection.6.3}}
\newlabel{section:data}{{6.3}{28}{Dataset Description}{subsection.6.3}{}}
\citation{neurodata,kiar2016graph}
\citation{athreya2013limit}
\citation{athreya2013limit}
\citation{brown1977means}
\bibstyle{plainnat}
\bibdata{Bib.bib}
\bibcite{neurodata}{{1}{}{{neu}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Outline for the Proof of the Theorems}{29}{subsection.6.4}}
\newlabel{section:outline_proof}{{6.4}{29}{Outline for the Proof of the Theorems}{subsection.6.4}{}}
\bibcite{ambroise2012new}{{2}{2012}{{Ambroise and Matias}}{{}}}
\bibcite{athreya2013limit}{{3}{2013}{{Athreya et~al.}}{{Athreya, Priebe, Tang, Lyzinski, Marchette, and Sussman}}}
\bibcite{bollobas2007phase}{{4}{2007}{{Bollob{\'a}s et~al.}}{{Bollob{\'a}s, Janson, and Riordan}}}
\bibcite{brown1977means}{{5}{1977}{{Brown and Rutemiller}}{{}}}
\bibcite{chatterjee2015matrix}{{6}{2015}{{Chatterjee}}{{}}}
\bibcite{choi2012stochastic}{{7}{2012}{{Choi et~al.}}{{Choi, Wolfe, and Airoldi}}}
\bibcite{daudin2008mixture}{{8}{2008}{{Daudin et~al.}}{{Daudin, Picard, and Robin}}}
\bibcite{durante2014nonparametric}{{9}{2014}{{Durante et~al.}}{{Durante, Dunson, and Vogelstein}}}
\bibcite{ginestet2014hypothesis}{{10}{2014}{{Ginestet et~al.}}{{Ginestet, Balanchandran, Rosenberg, and Kolaczyk}}}
\bibcite{gorgolewski2015high}{{11}{2015}{{Gorgolewski et~al.}}{{Gorgolewski, Mendes, Wilfling, Wladimirow, Gauthier, Bonnen, Ruby, Trampel, Bazin, Cozatl, et~al.}}}
\bibcite{gutmann1982stein}{{12}{1982}{{Gutmann}}{{}}}
\bibcite{hoff2002latent}{{13}{2002}{{Hoff et~al.}}{{Hoff, Raftery, and Handcock}}}
\bibcite{holland1983stochastic}{{14}{1983}{{Holland et~al.}}{{Holland, Laskey, and Leinhardt}}}
\bibcite{huber2009robust}{{15}{2009}{{Huber and Ronchetti}}{{}}}
\bibcite{james1961estimation}{{16}{1961}{{James and Stein}}{{}}}
\bibcite{jiang2001median}{{17}{2001}{{Jiang et~al.}}{{Jiang, M{\"u}unger, and Bunke}}}
\bibcite{kiar2016graph}{{18}{2016}{{Kiar}}{{}}}
\bibcite{kiar2016m2g}{{19}{In Preparation}{{Kiar et~al.}}{{Kiar, Roncal, Mhembere, Bridgeford, Burns, Priebe, and Vogelstein}}}
\bibcite{marchette2011vertex}{{20}{2011}{{Marchette et~al.}}{{Marchette, Priebe, and Coppersmith}}}
\bibcite{nickel2007random}{{21}{2007}{{Nickel}}{{}}}
\bibcite{pavlovic2014stochastic}{{22}{2014}{{Pavlovic et~al.}}{{Pavlovic, V{\'e}rtes, Bullmore, Schafer, and Nichols}}}
\bibcite{picard2009deciphering}{{23}{2009}{{Picard et~al.}}{{Picard, Miele, Daudin, Cottret, and Robin}}}
\bibcite{qin2013maximum}{{24}{2013}{{Qin and Priebe}}{{}}}
\bibcite{gray2013migraine}{{25}{2013}{{Roncal et~al.}}{{Roncal, Koterba, Mhembere, Kleissas, Vogelstein, Burns, Bowles, Donavos, Ryman, Jung, Wu, Calhoun, and Vogelstein}}}
\bibcite{scheinerman2010modeling}{{26}{2010}{{Scheinerman and Tucker}}{{}}}
\bibcite{stein1956inadmissibility}{{27}{1956}{{Stein}}{{}}}
\bibcite{sussman2014consistent}{{28}{2014}{{Sussman et~al.}}{{Sussman, Tang, and Priebe}}}
\bibcite{trunk1979problem}{{29}{1979}{{Trunk}}{{}}}
\bibcite{wolfe2013nonparametric}{{30}{2013}{{Wolfe and Olhede}}{{}}}
\bibcite{young2007random}{{31}{2007}{{Young and Scheinerman}}{{}}}
\bibcite{zanghi2008fast}{{32}{2008}{{Zanghi et~al.}}{{Zanghi, Ambroise, and Miele}}}
\bibcite{zanghi2010clustering}{{33}{2010}{{Zanghi et~al.}}{{Zanghi, Volant, and Ambroise}}}
\bibcite{zhu2006automatic}{{34}{2006}{{Zhu and Ghodsi}}{{}}}
\bibcite{zuo2014open}{{35}{2014}{{Zuo et~al.}}{{Zuo, Anderson, Bellec, Birn, Biswal, Blautzik, Breitner, Buckner, Calhoun, Castellanos, et~al.}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs for Theory Results}{32}{appendix.A}}
\citation{athreya2013limit}
\citation{athreya2013limit}
\newlabel{thm:clt_ext}{{A.1}{33}{Corrolary of Theorem 1 in \citet {athreya2013limit}}{fact.A.1}{}}
\newlabel{eq:4}{{3}{33}{Corrolary of Theorem 1 in \citet {athreya2013limit}}{equation.A.3}{}}
\newlabel{lm:mseForm}{{A.2}{33}{}{fact.A.2}{}}
\citation{athreya2013limit}
