\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{trunk1979problem}
\citation{stein1956inadmissibility}
\citation{james1961estimation}
\citation{gutmann1982stein}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{jiang2001median}
\citation{ginestet2014hypothesis}
\citation{bollobas2007phase}
\citation{holland1983stochastic}
\citation{hoff2002latent}
\citation{young2007random,nickel2007random}
\@writefile{toc}{\contentsline {section}{\numberline {2}Models}{3}{section.2}}
\newlabel{section:model}{{2}{3}{Models}{section.2}{}}
\citation{bollobas2007phase}
\citation{hoff2002latent}
\citation{young2007random,nickel2007random}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Independent Edge Model}{4}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Random Dot Product Graph}{4}{subsection.2.2}}
\citation{holland1983stochastic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Stochastic Block Model as a Random Dot Product Graph}{5}{subsection.2.3}}
\newlabel{section:sbm_rdpg}{{2.3}{5}{Stochastic Block Model as a Random Dot Product Graph}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Estimators}{5}{section.3}}
\newlabel{sec:estimator}{{3}{5}{Estimators}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Example illustrating the stochastic blockmodel.} The top left figure shows the mean graph $P$ with $K = 5$ blocks and $N=200$ vertices and the top right figure shows an adjacency matrix $A$ sampled according to the probabilities from $P$. While $A$ is a noisy version of $P$, much of the structure of $P$ is preserved in $A$, a property we will exploit in our estimation procedure. Based on three graphs sampled independently and identically according to the probability matrix $P$, we construct the element-wise mean $\mathaccentV {bar}016{A}$, shown in the lower right panel (see Section\nobreakspace  {}\ref  {sec:abar}). Finally, by taking a rank-5 approximation of $\mathaccentV {bar}016{A}$ and thresholding the values to be between $0$ and $1$, we construct our proposed estimate $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$, shown in the lower left panel (see Section\nobreakspace  {}\ref  {sec:phat}). By visual inspection, it is clear that the low-rank estimate $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ more closely approximates the probability matrix $P$ as compared to $\mathaccentV {bar}016{A}$. \relax }}{6}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SBM_example}{{1}{6}{{\bf Example illustrating the stochastic blockmodel.} The top left figure shows the mean graph $P$ with $K = 5$ blocks and $N=200$ vertices and the top right figure shows an adjacency matrix $A$ sampled according to the probabilities from $P$. While $A$ is a noisy version of $P$, much of the structure of $P$ is preserved in $A$, a property we will exploit in our estimation procedure. Based on three graphs sampled independently and identically according to the probability matrix $P$, we construct the element-wise mean $\bar {A}$, shown in the lower right panel (see Section~\ref {sec:abar}). Finally, by taking a rank-5 approximation of $\bar {A}$ and thresholding the values to be between $0$ and $1$, we construct our proposed estimate $\hat {P}$, shown in the lower left panel (see Section~\ref {sec:phat}). By visual inspection, it is clear that the low-rank estimate $\hat {P}$ more closely approximates the probability matrix $P$ as compared to $\bar {A}$. \relax }{figure.caption.1}{}}
\citation{chatterjee2015matrix}
\citation{sussman2014consistent}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Element-wise sample mean}{7}{subsection.3.1}}
\newlabel{sec:abar}{{3.1}{7}{Element-wise sample mean}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Low-Rank Estimator}{7}{subsection.3.2}}
\newlabel{sec:phat}{{3.2}{7}{Low-Rank Estimator}{subsection.3.2}{}}
\citation{zhu2006automatic}
\citation{chatterjee2015matrix}
\citation{scheinerman2010modeling}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Algorithm to compute the rank-$d$ approximation of a matrix.\relax }}{8}{algorithm.1}}
\newlabel{algo:lowrank}{{1}{8}{Algorithm to compute the rank-$d$ approximation of a matrix.\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Algorithm to compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$\relax }}{8}{algorithm.2}}
\newlabel{algo:basic}{{2}{8}{Algorithm to compute $\hat {P}$\relax }{algorithm.2}{}}
\citation{athreya2013limit}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{9}{section.4}}
\newlabel{sec:result}{{4}{9}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Asymptotic Theory}{9}{subsection.4.1}}
\newlabel{section:theoretical_result}{{4.1}{9}{Asymptotic Theory}{subsection.4.1}{}}
\newlabel{lm:VarPhat}{{4.1}{9}{}{fact.4.1}{}}
\newlabel{thm:ARE}{{4.2}{9}{}{fact.4.2}{}}
\newlabel{eq:sbm_are}{{4.2}{10}{}{fact.4.2}{}}
\newlabel{eq:approx_re}{{1}{10}{}{equation.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {\bf  Asymptotic scaled relative efficiency $N\cdot \mathrm  {RE}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$ in a two-block SBM.} For each distinct pair of edge probabilities in a two block model, the scaled relative efficiency only depends on the proportion of vertices in each block. We show the scaled asymptotic relative efficiency as $\rho _1$ changes from $0,1$ for pairs of vertices where either both are in block one or one is in block one and one is in block two. These curves all intersect at a scaled relative efficiency of 4 when $\rho _1=1/2=\rho _2$. Improvements using low-rank methods are greater for larger blocks, such as for $B_{11}$ when $\rho _1$ is close to 1, while the improvements are smaller for block pairs with relatively few vertex pairs such as $B_{11}$ when $\rho _1$ is small and $B_{12}$ when $\rho _1$ is near 0 or 1 . Note that the curve for $B_{22}$ would be the same as that for $B_{11}$ but reflected around the vertical line when $\rho _1=1/2$. Overall, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ performs best for large blocks while the improvements may be very minor for blocks with only a few vertices. \relax }}{11}{figure.caption.2}}
\newlabel{fig:RErho}{{2}{11}{{\bf Asymptotic scaled relative efficiency $N\cdot \mathrm {RE}(\bar {A},\hat {P})$ in a two-block SBM.} For each distinct pair of edge probabilities in a two block model, the scaled relative efficiency only depends on the proportion of vertices in each block. We show the scaled asymptotic relative efficiency as $\rho _1$ changes from $0,1$ for pairs of vertices where either both are in block one or one is in block one and one is in block two. These curves all intersect at a scaled relative efficiency of 4 when $\rho _1=1/2=\rho _2$. Improvements using low-rank methods are greater for larger blocks, such as for $B_{11}$ when $\rho _1$ is close to 1, while the improvements are smaller for block pairs with relatively few vertex pairs such as $B_{11}$ when $\rho _1$ is small and $B_{12}$ when $\rho _1$ is near 0 or 1 . Note that the curve for $B_{22}$ would be the same as that for $B_{11}$ but reflected around the vertical line when $\rho _1=1/2$. Overall, $\hat {P}$ performs best for large blocks while the improvements may be very minor for blocks with only a few vertices. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Finite Sample Simulations}{12}{subsection.4.2}}
\newlabel{sec:sbm_sim}{{4.2}{12}{Finite Sample Simulations}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces {\bf  Finite sample relative efficiency based on simulations. } The top panel shows the estimated relative efficiency $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {RE}$}\mathaccent "0362{\mathrm  {RE}}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$ as a function of $N$ for fixed $M=100$ based on simulations of an SBM. For each value of $N$, we used 1000 Monte Carlo replicates of the SBM from Section\nobreakspace  {}\ref  {sec:sbm_sim} to estimate the RE. Each curve corresponds to an average across vertex pairs corresponding to the three distinct block probabilities $B_{11}$, $B_{12}$, and $B_{22}$ in the two-block SBM. Recall that values below 1 indicate that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ is performing better than $\mathaccentV {bar}016{A}$. The relative efficiencies are all very close so the lines are indistinguishable.   To distinguish the three curves, the bottom panel shows the corresponding scaled relative efficiencies, $N\cdot \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {RE}$}\mathaccent "0362{\mathrm  {RE}}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$. The solid horizontal line indicates the theoretical asymptotic scaled relative which is $1/\rho _s+1/\rho _t=4$, since $\rho _1=\rho _2=4$. All the curves converge quickly to this theoretical limit. \relax }}{13}{figure.caption.3}}
\newlabel{fig:RE}{{3}{13}{{\bf Finite sample relative efficiency based on simulations. } The top panel shows the estimated relative efficiency $\hat {\mathrm {RE}}(\bar {A},\hat {P})$ as a function of $N$ for fixed $M=100$ based on simulations of an SBM. For each value of $N$, we used 1000 Monte Carlo replicates of the SBM from Section~\ref {sec:sbm_sim} to estimate the RE. Each curve corresponds to an average across vertex pairs corresponding to the three distinct block probabilities $B_{11}$, $B_{12}$, and $B_{22}$ in the two-block SBM. Recall that values below 1 indicate that $\hat {P}$ is performing better than $\bar {A}$. The relative efficiencies are all very close so the lines are indistinguishable. \\ To distinguish the three curves, the bottom panel shows the corresponding scaled relative efficiencies, $N\cdot \hat {\mathrm {RE}}(\bar {A},\hat {P})$. The solid horizontal line indicates the theoretical asymptotic scaled relative which is $1/\rho _s+1/\rho _t=4$, since $\rho _1=\rho _2=4$. All the curves converge quickly to this theoretical limit. \relax }{figure.caption.3}{}}
\citation{zuo2014open,gorgolewski2015high}
\citation{gray2013migraine}
\citation{kiar2016m2g}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}CoRR Brain Graphs: Cross-Validation}{14}{subsection.4.3}}
\newlabel{sec:corr_data}{{4.3}{14}{CoRR Brain Graphs: Cross-Validation}{subsection.4.3}{}}
\citation{zhu2006automatic}
\citation{chatterjee2015matrix}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\bf  Comparison of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ and $\mathaccentV {bar}016{A}$ for three atlases at three sample sizes for the CoRR data.} These plots show the mean squared error for $\mathaccentV {bar}016{A}$ (solid line) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ (dashed line) for three dataset (JHU, Desikan, and CPAC200) while embedding the graphs into different dimensions and with different sample sizes $M$. The average dimensions chosen by the 3rd elbow of Zhu and Ghodsi is denoted by a triangle and those chosen by USVT with threshold equaling 0.7 is denoted by a square. Vertical intervals, visible mainly in the $N=48,70$ and $M=1$ plots, represent the 95\% confidence interval for the mean squared errors. When $M$ is small, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ outperforms $\mathaccentV {bar}016{A}$ with a flexible range of the embedding dimension including the average of the dimensions selected by Zhu and Ghodsi and USVT.\relax }}{16}{figure.caption.4}}
\newlabel{fig:realdata}{{4}{16}{{\bf Comparison of $\hat {\mathrm {MSE}}$ of $\hat {P}$ and $\bar {A}$ for three atlases at three sample sizes for the CoRR data.} These plots show the mean squared error for $\bar {A}$ (solid line) and $\hat {P}$ (dashed line) for three dataset (JHU, Desikan, and CPAC200) while embedding the graphs into different dimensions and with different sample sizes $M$. The average dimensions chosen by the 3rd elbow of Zhu and Ghodsi is denoted by a triangle and those chosen by USVT with threshold equaling 0.7 is denoted by a square. Vertical intervals, visible mainly in the $N=48,70$ and $M=1$ plots, represent the 95\% confidence interval for the mean squared errors. When $M$ is small, $\hat {P}$ outperforms $\bar {A}$ with a flexible range of the embedding dimension including the average of the dimensions selected by Zhu and Ghodsi and USVT.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {\bf  Relative efficiencies of $\mathaccentV {bar}016{A}$ versus $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ for the CoRR data set.} For each atlas, JHU, Desikan, and CPAC 200, we sampled graphs which we used to compute $\mathaccentV {bar}016{A}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$. We compared different sample sizes $M$ and different dimension selection procedures, ZG and USVT. For each of the two methods for computing $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$, we estimated their relative efficiencies with respect to the sample mean $\mathaccentV {bar}016{A}$. Confidence intervals all had lengths less than $0.015$, and hence we omitted them for clarity. Overall, the relative efficiencies are greater for smaller sample sizes $M$ and larger number of vertices $N$. \relax }}{17}{table.caption.5}}
\newlabel{tab:corr_re}{{1}{17}{{\bf Relative efficiencies of $\bar {A}$ versus $\hat {P}$ for the CoRR data set.} For each atlas, JHU, Desikan, and CPAC 200, we sampled graphs which we used to compute $\bar {A}$ and $\hat {P}$. We compared different sample sizes $M$ and different dimension selection procedures, ZG and USVT. For each of the two methods for computing $\hat {P}$, we estimated their relative efficiencies with respect to the sample mean $\bar {A}$. Confidence intervals all had lengths less than $0.015$, and hence we omitted them for clarity. Overall, the relative efficiencies are greater for smaller sample sizes $M$ and larger number of vertices $N$. \relax }{table.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\bf  Heat maps of the population mean, the sample mean, and the estimator $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$.} These heat maps indicate the sample mean for the remaining $454-5$ graphs (left), sample mean for the 5 sampled graphs (center), and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ for the 5 sampled graphs with dimension $d=11$ selected using the Zhu and Ghodsi method (right). Darker pixels indicate a higher probability of an edge between the given vertices. Note that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ appears to better estimate the true probability matrix $P$, especially for edges between the two hemispheres, in the upper right and corresponding lower left block. \relax }}{18}{figure.caption.6}}
\newlabel{fig:Matrix_desikan_m5}{{5}{18}{{\bf Heat maps of the population mean, the sample mean, and the estimator $\hat {P}$.} These heat maps indicate the sample mean for the remaining $454-5$ graphs (left), sample mean for the 5 sampled graphs (center), and $\hat {P}$ for the 5 sampled graphs with dimension $d=11$ selected using the Zhu and Ghodsi method (right). Darker pixels indicate a higher probability of an edge between the given vertices. Note that $\hat {P}$ appears to better estimate the true probability matrix $P$, especially for edges between the two hemispheres, in the upper right and corresponding lower left block. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Synthetic Data Analysis for Full Rank IEM}{18}{subsection.4.4}}
\newlabel{sec:sim_iem}{{4.4}{18}{Synthetic Data Analysis for Full Rank IEM}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces {\bf  Heat plot of absolute estimation error for $\bm  {\mathaccentV {bar}016{A}}$ and $\bm  {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}}$ (lower triangle) and absolute errors above 0.4 (upper triangle).} These heat plots show the absolute estimation error $|\mathaccentV {bar}016{A} - P|$ and $|\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P} - P|$ for a sample of size $M=5$ from the Desikan dataset. The embedding dimension for $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ is $d=11$ selected by the 3rd elbow of the ZG method. The lower triangular matrix shows the actual absolute difference, while the upper triangular matrix only highlights the edges with absolute differences larger than $0.4$. The fact that 18 edges from $\mathaccentV {bar}016{A}$ are highlighted and only six edges from $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ are highlighted indicates that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ has fewer large outliers compared to $\mathaccentV {bar}016{A}$. \relax }}{19}{figure.caption.7}}
\newlabel{fig:Diff_desikan_m5}{{6}{19}{{\bf Heat plot of absolute estimation error for $\bm {\bar {A}}$ and $\bm {\hat {P}}$ (lower triangle) and absolute errors above 0.4 (upper triangle).} These heat plots show the absolute estimation error $|\bar {A} - P|$ and $|\hat {P} - P|$ for a sample of size $M=5$ from the Desikan dataset. The embedding dimension for $\hat {P}$ is $d=11$ selected by the 3rd elbow of the ZG method. The lower triangular matrix shows the actual absolute difference, while the upper triangular matrix only highlights the edges with absolute differences larger than $0.4$. The fact that 18 edges from $\bar {A}$ are highlighted and only six edges from $\hat {P}$ are highlighted indicates that $\hat {P}$ has fewer large outliers compared to $\bar {A}$. \relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\bf  Top 5 regions of the brain (vertices in graphs) and top 50 connections between regions (edges in graphs) with the largest differences $\bm  {|\mathaccentV {bar}016{A}_{ij} - P_{ij}| - |\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}_{ij} - P_{ij}|}$.} Red edges indicate that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ overestimate $P$ while blue means that $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ underestimates $P$. The edge width is determined by the estimation error. Connections with larger estimation error are represented by thicker lines. This figure shows the regions and connections of the brain where $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ outperforms $\mathaccentV {bar}016{A}$ the most for estimating $P$.\relax }}{19}{figure.caption.8}}
\newlabel{fig:Diff_between_desikan}{{7}{19}{{\bf Top 5 regions of the brain (vertices in graphs) and top 50 connections between regions (edges in graphs) with the largest differences $\bm {|\bar {A}_{ij} - P_{ij}| - |\hat {P}_{ij} - P_{ij}|}$.} Red edges indicate that $\hat {P}$ overestimate $P$ while blue means that $\hat {P}$ underestimates $P$. The edge width is determined by the estimation error. Connections with larger estimation error are represented by thicker lines. This figure shows the regions and connections of the brain where $\hat {P}$ outperforms $\bar {A}$ the most for estimating $P$.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\bf  Comparison of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ and $\mathaccentV {bar}016{A}$ for synthetic data analysis.} As in Fig.\nobreakspace  {}\ref  {fig:RE}, this figure shows $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {MSE}$}\mathaccent "0362{\mathrm  {MSE}}$ for $\mathaccentV {bar}016{A}$ (solid line) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P}$ (dashed line) for simulated data with different sample sizes $M$ based on the sample mean for the Desikan dataset. Again, the average of dimensions selected by the USVT method (square) and the ZG method (triangle) tend to nearly approximate the optimal dimension. Overall, we see that the structure of these plots well approximates the structure for the real data indicating that performance for the independent edge model will tend to translate in structure to non-independent edge scenarios. On the other hand, the relative efficiency $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \mathrm  {RE}$}\mathaccent "0362{\mathrm  {RE}}(\mathaccentV {bar}016{A},\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle P$}\mathaccent "0362{P})$ is lower for this synthetic data analysis than for the CoRR data.\relax }}{20}{figure.caption.9}}
\newlabel{fig:sim_desikan}{{8}{20}{{\bf Comparison of $\hat {P}$ and $\bar {A}$ for synthetic data analysis.} As in Fig.~\ref {fig:RE}, this figure shows $\hat {\mathrm {MSE}}$ for $\bar {A}$ (solid line) and $\hat {P}$ (dashed line) for simulated data with different sample sizes $M$ based on the sample mean for the Desikan dataset. Again, the average of dimensions selected by the USVT method (square) and the ZG method (triangle) tend to nearly approximate the optimal dimension. Overall, we see that the structure of these plots well approximates the structure for the real data indicating that performance for the independent edge model will tend to translate in structure to non-independent edge scenarios. On the other hand, the relative efficiency $\hat {\mathrm {RE}}(\bar {A},\hat {P})$ is lower for this synthetic data analysis than for the CoRR data.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{20}{section.5}}
\newlabel{sec:discussion}{{5}{20}{Discussion}{section.5}{}}
\citation{durante2014nonparametric}
\citation{huber2009robust,qin2013maximum}
\citation{zhu2006automatic}
\citation{chatterjee2015matrix}
\citation{marchette2011vertex}
\citation{scheinerman2010modeling}
\@writefile{toc}{\contentsline {section}{\numberline {6}Methods}{22}{section.6}}
\newlabel{sec:method}{{6}{22}{Methods}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Choosing Dimension}{22}{subsection.6.1}}
\newlabel{section:dim_select}{{6.1}{22}{Choosing Dimension}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Graph Diagonal Augmentation}{22}{subsection.6.2}}
\newlabel{section:diag_aug}{{6.2}{22}{Graph Diagonal Augmentation}{subsection.6.2}{}}
\citation{zuo2014open,gorgolewski2015high}
\citation{neurodata,kiar2016graph}
\citation{athreya2013limit}
\citation{athreya2013limit}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Mean Squared Error and Relative Efficiency}{23}{subsection.6.3}}
\newlabel{section:rel_eff}{{6.3}{23}{Mean Squared Error and Relative Efficiency}{subsection.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Dataset Description}{23}{subsection.6.4}}
\newlabel{section:data}{{6.4}{23}{Dataset Description}{subsection.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Outline for the Proof of the Theorems}{23}{subsection.6.5}}
\newlabel{section:outline_proof}{{6.5}{23}{Outline for the Proof of the Theorems}{subsection.6.5}{}}
\citation{brown1977means}
\bibstyle{plainnat}
\bibdata{Bib.bib}
\bibcite{neurodata}{{1}{}{{neu}}{{}}}
\bibcite{athreya2013limit}{{2}{2013}{{Athreya et~al.}}{{Athreya, Priebe, Tang, Lyzinski, Marchette, and Sussman}}}
\bibcite{bollobas2007phase}{{3}{2007}{{Bollob{\'a}s et~al.}}{{Bollob{\'a}s, Janson, and Riordan}}}
\bibcite{brown1977means}{{4}{1977}{{Brown and Rutemiller}}{{}}}
\bibcite{chatterjee2015matrix}{{5}{2015}{{Chatterjee}}{{}}}
\bibcite{durante2014nonparametric}{{6}{2014}{{Durante et~al.}}{{Durante, Dunson, and Vogelstein}}}
\bibcite{ginestet2014hypothesis}{{7}{2014}{{Ginestet et~al.}}{{Ginestet, Balanchandran, Rosenberg, and Kolaczyk}}}
\bibcite{gorgolewski2015high}{{8}{2015}{{Gorgolewski et~al.}}{{Gorgolewski, Mendes, Wilfling, Wladimirow, Gauthier, Bonnen, Ruby, Trampel, Bazin, Cozatl, et~al.}}}
\bibcite{gutmann1982stein}{{9}{1982}{{Gutmann}}{{}}}
\bibcite{hoff2002latent}{{10}{2002}{{Hoff et~al.}}{{Hoff, Raftery, and Handcock}}}
\bibcite{holland1983stochastic}{{11}{1983}{{Holland et~al.}}{{Holland, Laskey, and Leinhardt}}}
\bibcite{huber2009robust}{{12}{2009}{{Huber and Ronchetti}}{{}}}
\bibcite{james1961estimation}{{13}{1961}{{James and Stein}}{{}}}
\bibcite{jiang2001median}{{14}{2001}{{Jiang et~al.}}{{Jiang, M{\"u}unger, and Bunke}}}
\bibcite{kiar2016graph}{{15}{2016}{{Kiar}}{{}}}
\bibcite{kiar2016m2g}{{16}{In Preparation}{{Kiar et~al.}}{{Kiar, Roncal, Mhembere, Bridgeford, Burns, Priebe, and Vogelstein}}}
\bibcite{marchette2011vertex}{{17}{2011}{{Marchette et~al.}}{{Marchette, Priebe, and Coppersmith}}}
\bibcite{nickel2007random}{{18}{2007}{{Nickel}}{{}}}
\bibcite{qin2013maximum}{{19}{2013}{{Qin and Priebe}}{{}}}
\bibcite{gray2013migraine}{{20}{2013}{{Roncal et~al.}}{{Roncal, Koterba, Mhembere, Kleissas, Vogelstein, Burns, Bowles, Donavos, Ryman, Jung, Wu, Calhoun, and Vogelstein}}}
\bibcite{scheinerman2010modeling}{{21}{2010}{{Scheinerman and Tucker}}{{}}}
\bibcite{stein1956inadmissibility}{{22}{1956}{{Stein}}{{}}}
\bibcite{sussman2014consistent}{{23}{2014}{{Sussman et~al.}}{{Sussman, Tang, and Priebe}}}
\bibcite{trunk1979problem}{{24}{1979}{{Trunk}}{{}}}
\bibcite{young2007random}{{25}{2007}{{Young and Scheinerman}}{{}}}
\bibcite{zhu2006automatic}{{26}{2006}{{Zhu and Ghodsi}}{{}}}
\bibcite{zuo2014open}{{27}{2014}{{Zuo et~al.}}{{Zuo, Anderson, Bellec, Birn, Biswal, Blautzik, Breitner, Buckner, Calhoun, Castellanos, et~al.}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs for Theory Results}{26}{appendix.A}}
\citation{athreya2013limit}
\citation{athreya2013limit}
\newlabel{thm:clt_ext}{{A.1}{27}{Corrolary of Theorem 1 in \citet {athreya2013limit}}{fact.A.1}{}}
\newlabel{eq:4}{{2}{27}{Corrolary of Theorem 1 in \citet {athreya2013limit}}{equation.A.2}{}}
\newlabel{lm:mseForm}{{A.2}{27}{}{fact.A.2}{}}
\citation{athreya2013limit}
